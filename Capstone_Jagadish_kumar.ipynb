{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHSKJXuX4zUOw/VlPdoLkz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jagadish19k/retail-sales-forecasting/blob/main/Capstone_Jagadish_kumar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Capstone Project â€” Retail Sales Insights & Demand Forecasting**\n",
        "\n",
        "**Author:** Jagadish Kumar  \n",
        "**Date:** 2025-11-30\n",
        "\n",
        "This notebook implements the full workflow: data loading, cleaning, EDA,\n",
        "statistical tests, regression modeling, time-series forecasting, and\n",
        "PowerPoint generation. Visuals and outputs are saved to `/content`.\n"
      ],
      "metadata": {
        "id": "RfC2LW67SeXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ¦ Day 1 â€” Data Loading, Merging & Cleaning\n",
        "\n",
        "### ðŸ”¹ 1.1 Import Libraries  \n",
        "### ðŸ”¹ 1.2 Load CSV Files  \n",
        "### ðŸ”¹ 1.3 Merge train + stores + features  \n",
        "### ðŸ”¹ 1.4 Handle Missing Values  \n",
        "### ðŸ”¹ 1.5 Feature Engineering (Year, Month, Week, Holiday Flag)  \n",
        "### ðŸ”¹ 1.6 Save Cleaned Dataset  \n"
      ],
      "metadata": {
        "id": "UpRFuzhTii-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup & imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "\n",
        "DATA_DIR = Path('/content')  # Colab working directory\n",
        "TRAIN_CSV = DATA_DIR / 'train.csv'\n",
        "STORES_CSV = DATA_DIR / 'stores.csv'\n",
        "FEATURES_CSV = DATA_DIR / 'features.csv'\n",
        "\n",
        "print(\"Data path checks:\", TRAIN_CSV.exists(), STORES_CSV.exists(), FEATURES_CSV.exists())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw53NQ9aSo5t",
        "outputId": "69b08c5f-1ca1-4f4f-bedc-2c4f8261b4c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data path checks: False False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train = pd.read_csv(TRAIN_CSV, parse_dates=['Date'])\n",
        "stores = pd.read_csv(STORES_CSV)\n",
        "features = pd.read_csv(FEATURES_CSV, parse_dates=['Date'])\n",
        "\n",
        "print(\"Shapes ->\", train.shape, stores.shape, features.shape)\n",
        "display(train.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "rvL8vHnHTCFD",
        "outputId": "67c41144-8a6a-4fb0-e988-36d2f9f44b20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2725108785.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_CSV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTORES_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURES_CSV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and basic cleaning\n",
        "df = train.merge(stores, on='Store', how='left').merge(features, on=['Store','Date'], how='left')\n",
        "print(\"Merged shape:\", df.shape)\n",
        "\n",
        "# Ensure Date is datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Fill numeric missing with median\n",
        "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "for c in num_cols:\n",
        "    if df[c].isnull().any():\n",
        "        df[c] = df[c].fillna(df[c].median())\n",
        "\n",
        "# Fill object missing with 'Unknown'\n",
        "for c in df.select_dtypes(include=['object']).columns.tolist():\n",
        "    df[c] = df[c].fillna('Unknown')\n",
        "\n",
        "# Derived features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Week'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "# Ensure holiday flag exists\n",
        "if 'IsHoliday' not in df.columns:\n",
        "    if 'Holiday_Flag' in df.columns:\n",
        "        df['IsHoliday'] = df['Holiday_Flag'].astype(int)\n",
        "    else:\n",
        "        df['IsHoliday'] = 0\n",
        "else:\n",
        "    df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "# Save cleaned dataset\n",
        "out_clean = DATA_DIR / 'Cleaned_Merged.csv'\n",
        "df.to_csv(out_clean, index=False)\n",
        "print(\"Saved cleaned data to\", out_clean)\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "d1__kSS0TPyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ© Day 2 â€” Exploratory Data Analysis (EDA)\n",
        "\n",
        "### ðŸ”¹ 2.1 Top 10 Stores by Sales  \n",
        "### ðŸ”¹ 2.2 Monthly Sales Trend  \n",
        "### ðŸ”¹ 2.3 Correlation Analysis  \n",
        "### ðŸ”¹ 2.4 Holiday vs Non-Holiday Boxplot  \n",
        "### ðŸ”¹ 2.5 Add Insights Below Each Chart  \n"
      ],
      "metadata": {
        "id": "0dGJ8irHlrkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 stores by total Weekly_Sales\n",
        "if 'Weekly_Sales' in df.columns:\n",
        "    top_stores = df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False).head(10)\n",
        "    display(top_stores)\n",
        "    fig, ax = plt.subplots()\n",
        "    top_stores.plot(kind='bar', ax=ax)\n",
        "    ax.set_title('Top 10 Stores by Total Weekly Sales')\n",
        "    ax.set_ylabel('Total Weekly Sales')\n",
        "    fig.savefig(DATA_DIR/'top_10_stores.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Monthly chain-level trend\n",
        "if 'Weekly_Sales' in df.columns:\n",
        "    monthly = df.set_index('Date').resample('M')['Weekly_Sales'].sum()\n",
        "    fig, ax = plt.subplots()\n",
        "    monthly.plot(ax=ax)\n",
        "    ax.set_title('Monthly Sales Trend (Chain-level)')\n",
        "    ax.set_ylabel('Total Weekly Sales')\n",
        "    fig.savefig(DATA_DIR/'monthly_sales_trend.png', bbox_inches='tight')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "-hsBW515Tbch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation matrix (numeric)\n",
        "numeric = df.select_dtypes(include=['number'])\n",
        "if numeric.shape[1] > 1:\n",
        "    corr = numeric.corr()\n",
        "    fig, ax = plt.subplots(figsize=(10,8))\n",
        "    cax = ax.matshow(corr.fillna(0))\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticks(range(len(corr.columns)))\n",
        "    ax.set_yticks(range(len(corr.columns)))\n",
        "    ax.set_xticklabels(corr.columns, rotation=90)\n",
        "    ax.set_yticklabels(corr.columns)\n",
        "    ax.set_title('Correlation matrix (numeric)')\n",
        "    fig.savefig(DATA_DIR/'correlation_matrix.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Holiday vs non-holiday boxplot\n",
        "if 'Weekly_Sales' in df.columns:\n",
        "    fig, ax = plt.subplots(figsize=(8,4))\n",
        "    df.boxplot(column='Weekly_Sales', by='IsHoliday', ax=ax)\n",
        "    ax.set_title('Weekly Sales: Holiday (1) vs Non-Holiday (0)')\n",
        "    ax.set_xlabel('IsHoliday')\n",
        "    ax.set_ylabel('Weekly_Sales')\n",
        "    plt.suptitle('')\n",
        "    fig.savefig(DATA_DIR/'holiday_boxplot.png', bbox_inches='tight')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6SvsdB13ThmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ¨ Day 3 â€” Statistical Analysis\n",
        "\n",
        "### ðŸ”¹ 3.1 Holiday vs Non-Holiday (t-test & Mannâ€“Whitney)  \n",
        "### ðŸ”¹ 3.2 Store-Level Aggregated t-test  \n",
        "### ðŸ”¹ 3.3 Interpretation of Results  \n"
      ],
      "metadata": {
        "id": "4OpCm2ojl9cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "stat_results = {}\n",
        "\n",
        "# Count rows\n",
        "s_h = df[df['IsHoliday']==1]['Weekly_Sales']\n",
        "s_nh = df[df['IsHoliday']==0]['Weekly_Sales']\n",
        "\n",
        "# t-test (safe)\n",
        "if len(s_h)>1 and len(s_nh)>1:\n",
        "    t_stat, p_val = stats.ttest_ind(s_h, s_nh, equal_var=False)\n",
        "    stat_results['t_test'] = {'t':float(t_stat),'p':float(p_val)}\n",
        "else:\n",
        "    stat_results['t_test'] = \"Not enough data\"\n",
        "\n",
        "# Aggregated store-level t-test\n",
        "agg = df.groupby(['Store','IsHoliday'])['Weekly_Sales'].mean().reset_index()\n",
        "agg_h = agg[agg['IsHoliday']==1]['Weekly_Sales']\n",
        "agg_nh = agg[agg['IsHoliday']==0]['Weekly_Sales']\n",
        "\n",
        "if len(agg_h)>1 and len(agg_nh)>1:\n",
        "    t2, p2 = stats.ttest_ind(agg_h, agg_nh, equal_var=False)\n",
        "    stat_results['aggregated_ttest'] = {'t':float(t2),'p':float(p2)}\n",
        "else:\n",
        "    stat_results['aggregated_ttest'] = \"Not enough aggregated data\"\n",
        "\n",
        "# Mannâ€“Whitney U test\n",
        "if len(s_h)>0 and len(s_nh)>0:\n",
        "    u, p = stats.mannwhitneyu(s_h, s_nh)\n",
        "    stat_results['mann_whitney'] = {'u':float(u),'p':float(p)}\n",
        "else:\n",
        "    stat_results['mann_whitney'] = \"Not enough samples\"\n",
        "\n",
        "stat_results\n"
      ],
      "metadata": {
        "id": "g0dguMceUzjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ§ Day 4 â€” Predictive Modeling & Forecasting\n",
        "\n",
        "### ðŸ”¹ 4.1 Regression Model  \n",
        "### ðŸ”¹ 4.2 RÂ², MAE, RMSE  \n",
        "### ðŸ”¹ 4.3 Top Feature Coefficients  \n",
        "### ðŸ”¹ 4.4 Holtâ€“Winters 12-Week Forecast  \n",
        "### ðŸ”¹ 4.5 Save Forecast Plot  \n"
      ],
      "metadata": {
        "id": "E_vR7tMcnAUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression model to predict Weekly_Sales\n",
        "model_metrics = {}\n",
        "\n",
        "if 'Weekly_Sales' in df.columns:\n",
        "    X = df.select_dtypes(include=['number']).drop(columns=['Weekly_Sales'], errors='ignore')\n",
        "    y = df['Weekly_Sales'].fillna(0)\n",
        "\n",
        "    if X.shape[1] > 0:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X.fillna(0), y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        lr = LinearRegression().fit(X_train, y_train)\n",
        "        y_pred = lr.predict(X_test)\n",
        "\n",
        "        # Compute RMSE manually (for older sklearn)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = mse ** 0.5\n",
        "\n",
        "        model_metrics = {\n",
        "            'R2': float(r2_score(y_test, y_pred)),\n",
        "            'MAE': float(mean_absolute_error(y_test, y_pred)),\n",
        "            'RMSE': float(rmse)\n",
        "        }\n",
        "\n",
        "        # Save top 10 coefficients\n",
        "        coef_series = pd.Series(lr.coef_, index=X.columns).abs().sort_values(ascending=False)\n",
        "        coef_series.head(10).to_csv('/content/model_top_coefs.csv')\n",
        "\n",
        "        print(\"Model Metrics:\", model_metrics)\n",
        "    else:\n",
        "        print(\"No numeric features available for regression\")\n"
      ],
      "metadata": {
        "id": "D5Rt4WguU-F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "ts = df.set_index('Date').resample('W')['Weekly_Sales'].sum().dropna().sort_index()\n",
        "\n",
        "if len(ts) >= 52:\n",
        "    model = ExponentialSmoothing(ts, seasonal='add', seasonal_periods=52).fit()\n",
        "    fc = model.forecast(12)\n",
        "else:\n",
        "    # fallback if data insufficient\n",
        "    last_val = ts.iloc[-1]\n",
        "    fc = pd.Series([last_val]*12, index=pd.date_range(ts.index[-1], periods=12, freq='W'))\n",
        "\n",
        "forecast_df = fc.reset_index()\n",
        "forecast_df.columns = ['Date','Forecast_Weekly_Sales']\n",
        "forecast_df.to_csv('/content/ts_forecast.csv', index=False)\n",
        "\n",
        "# Plot forecast\n",
        "fig, ax = plt.subplots(figsize=(10,4))\n",
        "ts.plot(ax=ax, label='History')\n",
        "fc.plot(ax=ax, label='Forecast')\n",
        "ax.set_title('Weekly Sales Forecast (12 Weeks)')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n",
        "print(\"Forecast saved to ts_forecast.csv\")\n",
        "display(forecast_df.head())\n"
      ],
      "metadata": {
        "id": "e4bJqc_7VHqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "final_stats = {\n",
        "    'model_metrics': model_metrics,\n",
        "    'stat_tests': stat_results,\n",
        "    'generated_on': datetime.utcnow().isoformat()\n",
        "}\n",
        "\n",
        "with open('/content/model_metrics.json','w') as f:\n",
        "    json.dump(final_stats, f, indent=2)\n",
        "\n",
        "print(\"Saved model metrics to model_metrics.json\")\n"
      ],
      "metadata": {
        "id": "2eg2cyp2VRyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŸ« Day 5 â€” Insights, Recommendations & PPT Generation\n",
        "\n",
        "### ðŸ”¹ 5.1 Key Insights  \n",
        "### ðŸ”¹ 5.2 Executive Summary  \n",
        "### ðŸ”¹ 5.3 Final Recommendations  \n",
        "### ðŸ”¹ 5.4 Generate Enhanced PPT with Images  \n",
        "### ðŸ”¹ 5.5 Submission Checklist  \n"
      ],
      "metadata": {
        "id": "OGOs-wycmiKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install python-pptx (needed to generate PPTX)\n",
        "!pip install python-pptx\n"
      ],
      "metadata": {
        "id": "zCDw4HramrRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure to run: !pip install python-pptx before this cell\n",
        "\n",
        "try:\n",
        "    from pptx import Presentation\n",
        "    from pptx.util import Inches\n",
        "\n",
        "    prs = Presentation()\n",
        "\n",
        "    # Title slide\n",
        "    slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
        "    slide.shapes.title.text = \"Capstone Project: Sales Insights & Forecasting\"\n",
        "    slide.placeholders[1].text = f\"Author: Jagadish Kumar\\nGenerated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}\"\n",
        "\n",
        "    # Key Metrics slide\n",
        "    slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
        "    slide.shapes.title.text = \"Key Model Metrics\"\n",
        "    tf = slide.shapes.placeholders[1].text_frame\n",
        "    tf.text = str(model_metrics)\n",
        "\n",
        "    # Add EDA images\n",
        "    images = [\n",
        "        'top_10_stores.png',\n",
        "        'monthly_sales_trend.png',\n",
        "        'correlation_matrix.png',\n",
        "        'holiday_boxplot.png',\n",
        "        'ts_forecast_plot.png'\n",
        "    ]\n",
        "\n",
        "    for img in images:\n",
        "        path = Path('/content')/img\n",
        "        if path.exists():\n",
        "            slide = prs.slides.add_slide(prs.slide_layouts[5])\n",
        "            slide.shapes.title.text = img.replace('_',' ').title()\n",
        "            slide.shapes.add_picture(str(path), Inches(1), Inches(1.5), height=Inches(4.5))\n",
        "\n",
        "    # Summary slide\n",
        "    slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
        "    slide.shapes.title.text = \"Executive Summary\"\n",
        "    tf = slide.shapes.placeholders[1].text_frame\n",
        "    tf.text = (\n",
        "        \"â€¢ Identified store performance patterns\\n\"\n",
        "        \"â€¢ Evaluated holiday vs non-holiday sales\\n\"\n",
        "        \"â€¢ Built regression model for weekly sales\\n\"\n",
        "        \"â€¢ Produced 12-week demand forecast\\n\"\n",
        "        \"â€¢ Recommendations: inventory planning, staffing, promotions\"\n",
        "    )\n",
        "\n",
        "    ppt_path = \"/content/Capstone_Report.pptx\"\n",
        "    prs.save(ppt_path)\n",
        "    print(\"PowerPoint saved to:\", ppt_path)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"PPTX creation failed:\", e)\n"
      ],
      "metadata": {
        "id": "lg4biIrHVdwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-9Q1QK_V39l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}